{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8_43unDo-sb"
   },
   "source": [
    "이 자료는 위키독스 딥 러닝을 이용한 자연어 처리 입문의 허깅페이스 토크나이저 학습 자료입니다.  \n",
    "\n",
    "링크 : https://wikidocs.net/99893"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리 스타트업 허깅페이스가 개발한 패키지 tokenizers는 자주 등장하는 서브워드들을 하나의 토큰으로 취급하는 다양한 서브워드 토크나이저를 제공합니다. 이번 실습에서는 이 중에서 WordPiece Tokenizer를 실습해보겠습니다. 실습을 위해 우선 tokenizers를 설치합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM5ahbJ5pHYp"
   },
   "source": [
    "# 1. BERT의 워드피스 토크나이저(BertWordPieceTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:02.722340Z",
     "start_time": "2024-06-25T14:02:02.714447Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OuyGviaGpdTC",
    "outputId": "85c6e8e0-dde4-4740-ad34-d143916c486e"
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:03.604679Z",
     "start_time": "2024-06-25T14:02:03.525418Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "I6y5vrE1qyH-",
    "outputId": "f52ea60c-1df1-43bf-eade-277a90745eb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:06.808272Z",
     "start_time": "2024-06-25T14:02:04.442309Z"
    },
    "id": "PjIo8gG7pir-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:10.425611Z",
     "start_time": "2024-06-25T14:02:06.810003Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yy6bJkk-qEqG",
    "outputId": "309cae15-b383-4c4c-8a69-665a46f74cbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x24a558f97f0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:10.839747Z",
     "start_time": "2024-06-25T14:02:10.426954Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "eDRGo02FqEz9",
    "outputId": "4439e64c-e22b-456d-f4bc-2fe8079c12f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver_df = pd.read_table('ratings.txt')\n",
    "naver_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:23.738850Z",
     "start_time": "2024-06-25T14:02:23.686691Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuNq2qiHqWK7",
    "outputId": "77c7bab1-1f83-4c5c-bca1-e41374312dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "naver_df = naver_df.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(naver_df.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:34.538271Z",
     "start_time": "2024-06-25T14:02:34.414244Z"
    },
    "id": "R6oyRgzuqG0O"
   },
   "outputs": [],
   "source": [
    "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(naver_df['document']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글이 공개한 딥 러닝 모델 BERT에는 WordPiece Tokenizer가 사용되었습니다. 허깅페이스는 해당 토크나이저를 직접 구현하여 tokenizers라는 패키지를 통해 버트워드피스토크나이저(BertWordPieceTokenizer)를 제공합니다.\n",
    "\n",
    "여기서는 네이버 영화 리뷰 데이터를 해당 토크나이저에 학습시키고, 이로부터 서브워드의 단어 집합(Vocabulary)을 얻습니다. 그리고 임의의 문장에 대해서 학습된 토크나이저를 사용하여 토큰화를 진행합니다. 우선 네이버 영화 리뷰 데이터를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:35.059775Z",
     "start_time": "2024-06-25T14:02:35.041122Z"
    },
    "id": "UUxat59Kr_HN"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGak2nRzz1VF"
   },
   "source": [
    "lowercase : True일 경우 토크나이저는 영어의 대문자와 소문자를 동일한 문자 취급.  \n",
    "strip_accents : True일 경우 악센트 제거.  \n",
    " ex) é → e, ô → o  \n",
    "wordpieces_prefix : 서브워드로 쪼개졌을 경우 뒤의 서브워드에는 ##를 부착하여 원래 단어에서 분리된 것임을 표시.  \n",
    "  ex) 안녕하세요 -> [안녕, ##하세요]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:50.730087Z",
     "start_time": "2024-06-25T14:02:46.145541Z"
    },
    "id": "RqfckZaKsB52"
   },
   "outputs": [],
   "source": [
    "data_file = 'naver_review.txt'\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "tokenizer.train(files=data_file,\n",
    "                vocab_size=vocab_size,\n",
    "                limit_alphabet=limit_alphabet,\n",
    "                min_frequency=min_frequency,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jub515LugVN"
   },
   "source": [
    "vocab_size : 단어 집합의 크기  \n",
    "limit_alphabet : merge가 되지 않은 초기 토큰(character 단위)의 허용 제한 개수  \n",
    "min_frequency : merge가 되기 위한 pair의 최소 등장 횟수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:55.660502Z",
     "start_time": "2024-06-25T14:02:55.637233Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roBgrAOpvvR0",
    "outputId": "5dbba5eb-1b79-4556-9ea4-ac0d2d353117"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 저장\n",
    "tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:58.569857Z",
     "start_time": "2024-06-25T14:02:58.488891Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "fJ9bM4_twOK1",
    "outputId": "322f8c38-6abb-443e-ec0a-6e2aec5223cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>말안</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>말들이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>말라는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>말밖에는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>맘을</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0       [PAD]\n",
       "1       [UNK]\n",
       "2       [CLS]\n",
       "3       [SEP]\n",
       "4      [MASK]\n",
       "...       ...\n",
       "29995      말안\n",
       "29996     말들이\n",
       "29997     말라는\n",
       "29998    말밖에는\n",
       "29999      맘을\n",
       "\n",
       "[30000 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 로드\n",
    "df = pd.read_fwf('vocab.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:02:59.170816Z",
     "start_time": "2024-06-25T14:02:59.161250Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtXbcZHasGxO",
    "outputId": "52130d44-7ec6-4d07-e255-e5f8f9a4d42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['아', '배고', '##픈', '##데', '짜장면', '##먹고', '##싶다']\n",
      "정수 인코딩 : [2111, 20631, 3688, 3399, 24680, 7873, 7378]\n",
      "디코딩 : 아 배고픈데 짜장면먹고싶다\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('아 배고픈데 짜장면먹고싶다')\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:03:00.309246Z",
     "start_time": "2024-06-25T14:03:00.288274Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYQwqZajsqKs",
    "outputId": "eae9b700-9d5e-4cef-f4e9-567944ac7f73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['커피', '한잔', '##의', '여유', '##를', '즐기', '##다']\n",
      "정수 인코딩 : [12825, 25647, 3275, 12696, 3242, 10784, 3266]\n",
      "디코딩 : 커피 한잔의 여유를 즐기다\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('커피 한잔의 여유를 즐기다')\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA4GhI1apI5r"
   },
   "source": [
    "# 2. 기타 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:03:35.334341Z",
     "start_time": "2024-06-25T14:03:29.943453Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkgJ-USR4cg9",
    "outputId": "9dbfee4b-1c33-408d-f992-8dc2c3a8aed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁이', '▁영화는', '▁정말', '▁재미있', '습니다.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer\n",
    "                            \n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "encoded = tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:03:56.707981Z",
     "start_time": "2024-06-25T14:03:51.148823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이</w>', '영화는</w>', '정말</w>', '재미있습니다</w>', '.</w>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharBPETokenizer()\n",
    "tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "encoded = tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:04:09.868267Z",
     "start_time": "2024-06-25T14:04:01.112827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ìĿ´', 'ĠìĺģíĻĶëĬĶ', 'Ġìłķë§Ĳ', 'Ġìŀ¬ë¯¸ìŀĪìĬµëĭĪëĭ¤', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "encoded = tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\n",
    "print(encoded.tokens)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Wordpiece Tokenizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
